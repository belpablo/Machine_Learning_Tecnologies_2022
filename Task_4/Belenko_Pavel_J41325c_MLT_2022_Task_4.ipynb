{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5808b7",
   "metadata": {},
   "source": [
    "# Task 4:\n",
    "\n",
    "1.\tDownload Alice in Wonderland by Lewis Carroll from Project Gutenberg's website http://www.gutenberg.org/files/11/11-0.txt\n",
    "\n",
    "\n",
    "2.\tPerform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization.\n",
    "\n",
    "\n",
    "3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?\n",
    "\n",
    "\n",
    "4.\tFind the Top 10 most used verbs in sentences with Alice. What does Alice do most often?\n",
    "\n",
    "\n",
    "5.\t*(not necessary) Find Top 100 most used verbs in sentences with Alice. Get word vectors using a pre-trained word2vec model and visualize them. Compare the words using embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2bf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c61967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\belpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\belpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\belpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\belpa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b7cb1",
   "metadata": {},
   "source": [
    "# 1-2.\tDownload Alice in Wonderland by Lewis Carroll and Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers / non-alphabetic characters, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43b3a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org. If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook.\\n\\nTitle: Alice’s Adventures in Wonderland\\n\\nAuthor: Lewis Carroll\\n\\nRelease Date: January, 1991 [eBook #11]\\n[Most recently updated: October 12, 2020]\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\nProduced by: Arthur DiBianca and David Widger\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n CHAPTER I.     Down the Rabbit-Hole\\n CHAPTER II.    The Pool of Tears\\n CHAPTER III.   A Caucus-Race and a Long Tale\\n CHAPTER IV.    The Rabbit Sends in a Little Bill\\n CHAPTER V.     Advice from a Caterpillar\\n CHAPTER VI.    Pig and Pepper\\n CHAPTER VII.   A Mad Tea-Party\\n CHAPTER VIII.  The Queen’s Croquet-Ground\\n CHAPTER IX.    The Mock Turtle’s Story\\n CHAPTER X.     The Lobster Quadrille\\n CHAPTER XI.    Who Stole the Tarts?\\n CHAPTER XII.   Alice’s Evidence\\n\\n\\n\\n\\nCHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('AliceInWonderland.txt', 'r', encoding = 'utf-8')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb2c7056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh\\ndear! Oh dear! I shall be late!” (when she thought it over afterwards,\\nit occurred to her that she ought to have wondered at this, but at the\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\non, Alice started to her feet, for it flashed across her mind that she\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\nwatch to take out of it, and burning with curiosity, she ran across the\\nfield after it, and fortunately was just in time to see it pop down a\\nlarge rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very\\ndeep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had\\nplenty of time as she went down to look about her and to wonder what\\nwas going to happen next. First, she tried to look down and make out\\nwhat she was coming to, but it was too dark to see anything; then she\\nlooked at the sides of the well,'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting the header and the footer in text\n",
    "\n",
    "text = text.split('CHAPTER I.')[2]\n",
    "text = text.split('THE END')[0]\n",
    "\n",
    "text[:2000] # Now we got only text of book chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de0e8613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Down the RabbitHoleAlice was beginning to get very tired of sitting by her sister on thebank and of having nothing to do once or twice she had peeped intothe book her sister was reading but it had no pictures orconversations in it and what is the use of a book thought Alicewithout pictures or conversationsSo she was considering in her own mind as well as she could for thehot day made her feel very sleepy and stupid whether the pleasure ofmaking a daisychain would be worth the trouble of getting up andpicking the daisies when suddenly a White Rabbit with pink eyes ranclose by herThere was nothing so _very_ remarkable in that nor did Alice think itso _very_ much out of the way to hear the Rabbit say to itself Ohdear Oh dear I shall be late when she thought it over afterwardsit occurred to her that she ought to have wondered at this but at thetime it all seemed quite natural but when the Rabbit actually _took awatch out of its waistcoatpocket_ and looked at it and then hurriedon Alice started to her feet for it flashed across her mind that shehad never before seen a rabbit with either a waistcoatpocket or awatch to take out of it and burning with curiosity she ran across thefield after it and fortunately was just in time to see it pop down alarge rabbithole under the hedgeIn another moment down went Alice after it never once considering howin the world she was to get out againThe rabbithole went straight on like a tunnel for some way and thendipped suddenly down so suddenly that Alice had not a moment to thinkabout stopping herself before she found herself falling down a verydeep wellEither the well was very deep or she fell very slowly for she hadplenty of time as she went down to look about her and to wonder whatwas going to happen next First she tried to look down and make outwhat she was coming to but it was too dark to see anything then shelooked at the sides of the well and noticed that they were filled withcupboards and bookshelves here and there she saw maps an'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing extra characters with replacement\n",
    "\n",
    "text = re.sub('<.*?>', ' ', text)\n",
    "text = re.sub(' +', ' ', text)\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = re.sub('\\n', '', text)\n",
    "\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d7e24",
   "metadata": {},
   "source": [
    "# 3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"); how would you name each chapter according to the identified tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "986a4868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the text by the chapters\n",
    "\n",
    "chapters = text.split('CHAPTER')\n",
    "len(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afbfffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization:\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n",
    "text = text.lower()\n",
    "text = [lemmatizer.lemmatize(token) for token in TreebankWordTokenizer().tokenize(text)]\n",
    "text = [word for word in text if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90a74060",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in Chapter 1\n",
      "\tWord: was, TF-IDF: 0.14649\n",
      "\tWord: little, TF-IDF: 0.06388\n",
      "\tWord: like, TF-IDF: 0.05322\n",
      "\tWord: see, TF-IDF: 0.04594\n",
      "\tWord: bat, TF-IDF: 0.04386\n",
      "\tWord: way, TF-IDF: 0.04345\n",
      "\tWord: think, TF-IDF: 0.042\n",
      "\tWord: door, TF-IDF: 0.04134\n",
      "\tWord: nothing, TF-IDF: 0.04114\n",
      "\tWord: key, TF-IDF: 0.04098\n",
      "Top 10 words in Chapter 2\n",
      "\tWord: was, TF-IDF: 0.08917\n",
      "\tWord: mouse, TF-IDF: 0.06743\n",
      "\tWord: little, TF-IDF: 0.06666\n",
      "\tWord: dear, TF-IDF: 0.05204\n",
      "\tWord: cried, TF-IDF: 0.04989\n",
      "\tWord: foot, TF-IDF: 0.04908\n",
      "\tWord: pool, TF-IDF: 0.04769\n",
      "\tWord: must, TF-IDF: 0.04329\n",
      "\tWord: thing, TF-IDF: 0.04182\n",
      "\tWord: oh, TF-IDF: 0.04166\n",
      "Top 10 words in Chapter 3\n",
      "\tWord: said, TF-IDF: 0.12725\n",
      "\tWord: mouse, TF-IDF: 0.11355\n",
      "\tWord: dodo, TF-IDF: 0.08253\n",
      "\tWord: was, TF-IDF: 0.06269\n",
      "\tWord: know, TF-IDF: 0.04697\n",
      "\tWord: lory, TF-IDF: 0.04497\n",
      "\tWord: soon, TF-IDF: 0.044\n",
      "\tWord: long, TF-IDF: 0.03998\n",
      "\tWord: question, TF-IDF: 0.03839\n",
      "\tWord: course, TF-IDF: 0.03711\n",
      "Top 10 words in Chapter 4\n",
      "\tWord: was, TF-IDF: 0.06816\n",
      "\tWord: little, TF-IDF: 0.05823\n",
      "\tWord: rabbit, TF-IDF: 0.05657\n",
      "\tWord: bill, TF-IDF: 0.04188\n",
      "\tWord: puppy, TF-IDF: 0.04156\n",
      "\tWord: one, TF-IDF: 0.03907\n",
      "\tWord: heard, TF-IDF: 0.03829\n",
      "\tWord: said, TF-IDF: 0.03675\n",
      "\tWord: quite, TF-IDF: 0.03446\n",
      "\tWord: sure, TF-IDF: 0.03409\n",
      "Top 10 words in Chapter 5\n",
      "\tWord: said, TF-IDF: 0.16351\n",
      "\tWord: caterpillar, TF-IDF: 0.07539\n",
      "\tWord: was, TF-IDF: 0.0729\n",
      "\tWord: pigeon, TF-IDF: 0.05941\n",
      "\tWord: serpent, TF-IDF: 0.04621\n",
      "\tWord: size, TF-IDF: 0.04519\n",
      "\tWord: youth, TF-IDF: 0.04191\n",
      "\tWord: little, TF-IDF: 0.03924\n",
      "\tWord: father, TF-IDF: 0.03629\n",
      "\tWord: egg, TF-IDF: 0.03437\n",
      "Top 10 words in Chapter 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-96fa137dcf3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Top 10 words in Chapter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextCollection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0msorted_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-96fa137dcf3c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Top 10 words in Chapter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextCollection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchapter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0msorted_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\text.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m         \u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyConcatenation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idf_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\text.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tokens, name)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \"\"\"\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_COPY_TOKENS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\collections.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, chapter in enumerate(chapters):\n",
    "    \n",
    "    chapter = re.sub(r'[^\\w\\s]', '', chapter, re.UNICODE)\n",
    "    chapter = chapter.lower()\n",
    "    chapter = [lemmatizer.lemmatize(token) for token in TreebankWordTokenizer().tokenize(chapter)]\n",
    "    chapter = [word for word in chapter if not word in stop_words]\n",
    "    \n",
    "    analys = nltk.Text(chapter)\n",
    "    print('\\nTop 10 most important words from chapter {}:'.format(idx + 1)\n",
    "    \n",
    "    scores = {word: nltk.TextCollection(text).tf_idf(word, chapter) for word in analys}\n",
    "    sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse = True)\n",
    "    counter = 0\n",
    "    \n",
    "    for word, score in sorted_words[:11]:\n",
    "        if word == 'wa':\n",
    "            word = 'was'\n",
    "        if word != 'alice':\n",
    "            print(\"\\tWord: {}, TF-IDF metric: {}\".format(word, round(score, 5)))\n",
    "            counter += 1\n",
    "        if counter == 9:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0fac7",
   "metadata": {},
   "source": [
    "# 4. Find the Top 10 most used verbs in sentences with Alice. What does Alice do most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02d2e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('AliceInWonderland.txt', 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text = text.split('CHAPTER I.')[2]\n",
    "text = text.split('THE END')[0]\n",
    "text = re.sub('\\n', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9583d9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1638"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/31505798 split into sentences\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "splitted_text = split_into_sentences(text)\n",
    "len(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5baabd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_with_alice = []\n",
    "\n",
    "for sentence in splitted_text:\n",
    "    if re.search(r'\\bAlice\\b', sentence):\n",
    "        sentence_with_alice.append(sentence)\n",
    "        \n",
    "len(sentence_with_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bce4da65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'sit',\n",
       " 'have',\n",
       " 'do',\n",
       " 'twice',\n",
       " 'have',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'read',\n",
       " 'have',\n",
       " 'be',\n",
       " 'think']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "a = lambda x: lemmatizer.lemmatize(x[0], 'v')\n",
    "\n",
    "verbdict = []\n",
    "for sentence in sentence_with_alice:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    verbs = filter(lambda x:is_verb(x[1]),pos_tagged)\n",
    "    verbdict.append((list(map(a, list(verbs)))))\n",
    "    \n",
    "verbdict[0] # глаголы в первом предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afd44513",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'begin',\n",
       " 'get',\n",
       " 'sit',\n",
       " 'have',\n",
       " 'do',\n",
       " 'twice',\n",
       " 'have',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'read',\n",
       " 'have',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'do',\n",
       " 'think',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'think',\n",
       " 'occur',\n",
       " 'have',\n",
       " 'wonder',\n",
       " 'seem',\n",
       " '_took',\n",
       " 'look',\n",
       " 'hurriedon',\n",
       " 'start',\n",
       " 'flash',\n",
       " 'shehad',\n",
       " 'see',\n",
       " 'awatch',\n",
       " 'take',\n",
       " 'burn',\n",
       " 'run',\n",
       " 'be',\n",
       " 'see',\n",
       " 'pop',\n",
       " 'go',\n",
       " 'consider',\n",
       " 'howin',\n",
       " 'be',\n",
       " 'get',\n",
       " 'go',\n",
       " 'thendipped',\n",
       " 'have',\n",
       " 'thinkabout',\n",
       " 'stop',\n",
       " 'find',\n",
       " 'fall',\n",
       " 'think',\n",
       " 'herself',\n",
       " 'shallthink',\n",
       " 'tumble',\n",
       " 'Let',\n",
       " 'see',\n",
       " 'wouldbe',\n",
       " 'think',\n",
       " 'see',\n",
       " 'have',\n",
       " 'be',\n",
       " 'show',\n",
       " 'be',\n",
       " 'listen',\n",
       " 'be',\n",
       " 'say',\n",
       " 'wonder',\n",
       " 'get',\n",
       " 'have',\n",
       " 'noidea',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'say',\n",
       " 'be',\n",
       " 'do',\n",
       " 'begantalking',\n",
       " 'be',\n",
       " 'jump',\n",
       " 'look',\n",
       " 'be',\n",
       " 'be',\n",
       " 'be',\n",
       " 'hurry',\n",
       " 'be',\n",
       " 'be',\n",
       " 'lose',\n",
       " 'go',\n",
       " 'be',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'turn',\n",
       " 'get',\n",
       " 'come',\n",
       " 'make',\n",
       " 'be',\n",
       " 'sfirst',\n",
       " 'think',\n",
       " 'be',\n",
       " 'belong',\n",
       " 'open',\n",
       " 'find',\n",
       " 'lead',\n",
       " 'kneel',\n",
       " 'look',\n",
       " 'saw',\n",
       " 'long',\n",
       " 'getout',\n",
       " 'wander',\n",
       " 'get',\n",
       " 'go',\n",
       " 'be',\n",
       " 'see',\n",
       " 'hadhappened',\n",
       " 'have',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'be',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'wait',\n",
       " 'wentback',\n",
       " 'hop',\n",
       " 'find',\n",
       " 'shut',\n",
       " 'find',\n",
       " 'be',\n",
       " 'say',\n",
       " 'round',\n",
       " 'be',\n",
       " 'print',\n",
       " 'be',\n",
       " 'say',\n",
       " 'go',\n",
       " 'do',\n",
       " '_that_',\n",
       " 'be',\n",
       " 'venture',\n",
       " 'find',\n",
       " 'have',\n",
       " 'butter',\n",
       " 'finish',\n",
       " 'say',\n",
       " 'be',\n",
       " 'shut',\n",
       " 'shewaited',\n",
       " 'see',\n",
       " 'be',\n",
       " 'go',\n",
       " 'shrink',\n",
       " 'felt',\n",
       " 'end',\n",
       " 'know',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'go',\n",
       " 'find',\n",
       " 'happen',\n",
       " 'decide',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'advise',\n",
       " 'leave',\n",
       " 'think',\n",
       " 'pretend',\n",
       " 'be',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'surprise',\n",
       " 'thatfor',\n",
       " 'forget',\n",
       " 'speak',\n",
       " 'mopening',\n",
       " 'be',\n",
       " 'be',\n",
       " 'troublemyself',\n",
       " 'manage',\n",
       " 'but',\n",
       " 'bekind',\n",
       " 'win',\n",
       " 'walk',\n",
       " 'go',\n",
       " 'be',\n",
       " 'ashamed',\n",
       " 'say',\n",
       " 'say',\n",
       " 'go',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'ask',\n",
       " 'come',\n",
       " 'begin',\n",
       " 'please',\n",
       " 'start',\n",
       " 'drop',\n",
       " 'gloves',\n",
       " 'skurried',\n",
       " 'go',\n",
       " 'take',\n",
       " 'be',\n",
       " 'fan',\n",
       " 'go',\n",
       " 'talk',\n",
       " 'be',\n",
       " 'say',\n",
       " 'go',\n",
       " 'be',\n",
       " 'andI',\n",
       " 'have',\n",
       " 'go',\n",
       " 'live',\n",
       " 'have',\n",
       " 'play',\n",
       " 'cry',\n",
       " 'do',\n",
       " '_would_',\n",
       " 'put',\n",
       " 'say',\n",
       " 'frighten',\n",
       " 'find',\n",
       " 'andnow',\n",
       " 'have',\n",
       " 'be',\n",
       " 'have',\n",
       " 'come',\n",
       " 'goto',\n",
       " 'find',\n",
       " 'bath',\n",
       " 'dig',\n",
       " 'say',\n",
       " 'swim',\n",
       " 'find',\n",
       " 'be',\n",
       " 'think',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'have',\n",
       " 'do',\n",
       " 'remember',\n",
       " 'have',\n",
       " 'see',\n",
       " 'doesn',\n",
       " 'think',\n",
       " 'daresay',\n",
       " 'come',\n",
       " 'have',\n",
       " 'have',\n",
       " 'happen',\n",
       " 'cry',\n",
       " 'afraid',\n",
       " 'shehad',\n",
       " 'hurt',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'go',\n",
       " 'herself',\n",
       " 'swim',\n",
       " 'aboutin',\n",
       " 'sit',\n",
       " 'purr',\n",
       " 'lick',\n",
       " 'wash',\n",
       " 'be',\n",
       " 'catch',\n",
       " 'beg',\n",
       " 'cry',\n",
       " 'be',\n",
       " 'bristle',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'offend',\n",
       " 'say',\n",
       " 'change',\n",
       " 'do',\n",
       " 'go',\n",
       " 'be',\n",
       " 'like',\n",
       " 'show',\n",
       " 'cry',\n",
       " 'offend',\n",
       " 'turn',\n",
       " 'think',\n",
       " 'say',\n",
       " 'get',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'hate',\n",
       " 'be',\n",
       " 'get',\n",
       " 'have',\n",
       " 'seem',\n",
       " 'find',\n",
       " 'talk',\n",
       " 'have',\n",
       " 'know',\n",
       " 'have',\n",
       " 'say',\n",
       " 'amolder',\n",
       " 'know',\n",
       " 'allowwithout',\n",
       " 'know',\n",
       " 'be',\n",
       " 'refuse',\n",
       " 'totell',\n",
       " 'be',\n",
       " 'be',\n",
       " 'say',\n",
       " 'keep',\n",
       " 'fix',\n",
       " 'catch',\n",
       " 'do',\n",
       " 'get',\n",
       " 'continue',\n",
       " 'turn',\n",
       " 'speak',\n",
       " 'say',\n",
       " 'doesn',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'want',\n",
       " 'have',\n",
       " 'pause',\n",
       " 'think',\n",
       " 'tospeak',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'say',\n",
       " 'point',\n",
       " 'crowd',\n",
       " 'call',\n",
       " 'have',\n",
       " 'do',\n",
       " 'put',\n",
       " 'pull',\n",
       " 'get',\n",
       " 'hand',\n",
       " 'go',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'think',\n",
       " 'look',\n",
       " 'do',\n",
       " 'dare',\n",
       " 'think',\n",
       " 'say',\n",
       " 'bow',\n",
       " 'take',\n",
       " 'look',\n",
       " 'promise',\n",
       " 'tell',\n",
       " 'know',\n",
       " 'say',\n",
       " 'be',\n",
       " 'add',\n",
       " 'afraid',\n",
       " 'be',\n",
       " 'offend',\n",
       " 'say',\n",
       " 'turn',\n",
       " 'andsighing',\n",
       " '_is_',\n",
       " 'say',\n",
       " 'look',\n",
       " 'do',\n",
       " 'call',\n",
       " 'say',\n",
       " 'beg',\n",
       " 'say',\n",
       " 'have',\n",
       " 'get',\n",
       " 'think',\n",
       " 'say',\n",
       " 'make',\n",
       " 'plead',\n",
       " 'call',\n",
       " 'andthe',\n",
       " 'join',\n",
       " 'please',\n",
       " 'do',\n",
       " 'say',\n",
       " 'address',\n",
       " 'reply',\n",
       " 'be',\n",
       " 'talk',\n",
       " 'move',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'cry',\n",
       " 'felt',\n",
       " 'guess',\n",
       " 'be',\n",
       " 'look',\n",
       " 'begin',\n",
       " 'hunt',\n",
       " 'be',\n",
       " 'be',\n",
       " 'seeneverything',\n",
       " 'seem',\n",
       " 'have',\n",
       " 'change',\n",
       " 'have',\n",
       " 'vanish',\n",
       " 'go',\n",
       " 'hunt',\n",
       " 'andcalled',\n",
       " '_are_',\n",
       " 'youdoing',\n",
       " 'be',\n",
       " 'frighten',\n",
       " 'run',\n",
       " 'point',\n",
       " 'try',\n",
       " 'explain',\n",
       " 'themistake',\n",
       " 'have',\n",
       " 'make',\n",
       " 'queer',\n",
       " 'seem',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'be',\n",
       " 'go',\n",
       " 'fora',\n",
       " 'fancy',\n",
       " 'happen',\n",
       " 'go',\n",
       " 'stop',\n",
       " 'begin',\n",
       " 'order',\n",
       " 'have',\n",
       " 'have',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'thereseemed',\n",
       " 'be',\n",
       " 'get',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'think',\n",
       " 'talways',\n",
       " 'grow',\n",
       " 'be',\n",
       " 'order',\n",
       " 'think',\n",
       " '_never_',\n",
       " 'get',\n",
       " 'amnow',\n",
       " 'know',\n",
       " 'wasthe',\n",
       " 'come',\n",
       " 'look',\n",
       " 'tremble',\n",
       " 'shake',\n",
       " 'forget',\n",
       " 'be',\n",
       " 'have',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'come',\n",
       " 'try',\n",
       " 'open',\n",
       " 'open',\n",
       " 'be',\n",
       " 'press',\n",
       " 'prove',\n",
       " 'hear',\n",
       " 'say',\n",
       " 'get',\n",
       " 'think',\n",
       " 'wait',\n",
       " 'fanciedshe',\n",
       " 'hear',\n",
       " 'spread',\n",
       " 'make',\n",
       " 'be',\n",
       " 'hear',\n",
       " 'don',\n",
       " 'think',\n",
       " 'say',\n",
       " 'be',\n",
       " 'think',\n",
       " 'herself',\n",
       " '_will_',\n",
       " 'do',\n",
       " 'have',\n",
       " 'doubt',\n",
       " 'come',\n",
       " 'rattle',\n",
       " 'hit',\n",
       " 'notice',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'lay',\n",
       " 'come',\n",
       " 'make',\n",
       " 'sheappeared',\n",
       " 'run',\n",
       " 'find',\n",
       " 'get',\n",
       " 'do',\n",
       " 'say',\n",
       " 'herself',\n",
       " 'shewandered',\n",
       " 'be',\n",
       " 'grow',\n",
       " 'thesecond',\n",
       " 'be',\n",
       " 'find',\n",
       " 'say',\n",
       " 'coax',\n",
       " 'try',\n",
       " 'whistle',\n",
       " 'be',\n",
       " 'frighten',\n",
       " 'itmight',\n",
       " 'be',\n",
       " 'be',\n",
       " 'eat',\n",
       " 'know',\n",
       " 'do',\n",
       " 'pick',\n",
       " 'andheld',\n",
       " 'jump',\n",
       " 'offall',\n",
       " 'rush',\n",
       " 'make',\n",
       " 'worry',\n",
       " 'dodge',\n",
       " 'keep',\n",
       " 'be',\n",
       " 'run',\n",
       " 'appear',\n",
       " 'make',\n",
       " 'tumble',\n",
       " 'get',\n",
       " 'think',\n",
       " 'wasvery',\n",
       " 'have',\n",
       " 'expect',\n",
       " 'be',\n",
       " 'trample',\n",
       " 'run',\n",
       " 'begin',\n",
       " 'run',\n",
       " 'bark',\n",
       " 'sit',\n",
       " 'pant',\n",
       " 'tongue',\n",
       " 'hang',\n",
       " 'seem',\n",
       " 'Alice',\n",
       " 'make',\n",
       " 'sheset',\n",
       " 'run',\n",
       " 'be',\n",
       " 'till',\n",
       " 'sound',\n",
       " 'say',\n",
       " 'leantagainst',\n",
       " 'rest',\n",
       " 'fan',\n",
       " 'have',\n",
       " 'like',\n",
       " 'teach',\n",
       " 'trick',\n",
       " 'be',\n",
       " 'do',\n",
       " 'look',\n",
       " 'do',\n",
       " 'see',\n",
       " 'thatlooked',\n",
       " 'eat',\n",
       " 'drink',\n",
       " 'look',\n",
       " 'take',\n",
       " 'andaddressed',\n",
       " 'reply',\n",
       " 'know',\n",
       " 'knowwho',\n",
       " '_was_',\n",
       " 'get',\n",
       " 'think',\n",
       " 'have',\n",
       " 'beenchanged',\n",
       " 'say',\n",
       " 'see',\n",
       " 'put',\n",
       " 'reply',\n",
       " 'begin',\n",
       " 'be',\n",
       " 'be',\n",
       " 'haven',\n",
       " 'find',\n",
       " 'say',\n",
       " 'youhave',\n",
       " 'turn',\n",
       " 'knowand',\n",
       " 'thenafter',\n",
       " 'think',\n",
       " 'win',\n",
       " 'be',\n",
       " 'say',\n",
       " 'knowis',\n",
       " 'feel',\n",
       " '_me_',\n",
       " 'felt',\n",
       " 'make',\n",
       " 'draw',\n",
       " 'say',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'be',\n",
       " 'think',\n",
       " 'seem',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'sound',\n",
       " 'turn',\n",
       " 'come',\n",
       " 'say',\n",
       " 'swallow',\n",
       " 'think',\n",
       " 'have',\n",
       " 'do',\n",
       " 'tell',\n",
       " 'be',\n",
       " 'say',\n",
       " 'remember',\n",
       " 'don',\n",
       " 'keep',\n",
       " 'reply',\n",
       " 'fold',\n",
       " 'begin',\n",
       " 'be',\n",
       " 'say',\n",
       " 'have',\n",
       " 'become',\n",
       " 'stand',\n",
       " 'Do',\n",
       " 'think',\n",
       " 'be',\n",
       " '_quite_',\n",
       " 'say',\n",
       " 'have',\n",
       " 'get',\n",
       " 'alter',\n",
       " 'particular',\n",
       " 'reply',\n",
       " 'onedoesn',\n",
       " 'change',\n",
       " 'know',\n",
       " 'say',\n",
       " 'have',\n",
       " 'be',\n",
       " 'contradict',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'lose',\n",
       " 'like',\n",
       " 'be',\n",
       " 'wouldn',\n",
       " 'say',\n",
       " 'be',\n",
       " 'be',\n",
       " 'plead',\n",
       " 'wait',\n",
       " 'choose',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'remain',\n",
       " 'look',\n",
       " 'try',\n",
       " 'make',\n",
       " 'be',\n",
       " 'round',\n",
       " 'find',\n",
       " 'say',\n",
       " 'whichchanged',\n",
       " 'find',\n",
       " 'be',\n",
       " 'find',\n",
       " 'see',\n",
       " 'look',\n",
       " 'seem',\n",
       " 'rise',\n",
       " 'lay',\n",
       " 'say',\n",
       " 'say',\n",
       " 'haven',\n",
       " 're',\n",
       " 'talk',\n",
       " 'say',\n",
       " 'be',\n",
       " 'think',\n",
       " 'be',\n",
       " 'insaying',\n",
       " 'have',\n",
       " 'finish',\n",
       " 'be',\n",
       " 'annoy',\n",
       " 'say',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'say',\n",
       " 'm',\n",
       " 'say',\n",
       " 'rememberedthe',\n",
       " 'have',\n",
       " 'go',\n",
       " '_have_',\n",
       " 'taste',\n",
       " 'say',\n",
       " 'be',\n",
       " 'do',\n",
       " 'youknow',\n",
       " 'be',\n",
       " 'be',\n",
       " 'give',\n",
       " 'add',\n",
       " 'relooking',\n",
       " 'know',\n",
       " 'do',\n",
       " 'matter',\n",
       " 're',\n",
       " 'matter',\n",
       " '_me_',\n",
       " 'say',\n",
       " 'happen',\n",
       " 'be',\n",
       " 'shouldn',\n",
       " 'don',\n",
       " 'raw',\n",
       " 'crouch',\n",
       " 'keep',\n",
       " 'get',\n",
       " 'entangle',\n",
       " 'have',\n",
       " 'stop',\n",
       " 'untwist',\n",
       " 'live',\n",
       " 'think',\n",
       " 'll',\n",
       " 'do',\n",
       " 'come',\n",
       " 'frighten',\n",
       " 'be',\n",
       " 'open',\n",
       " 'notice',\n",
       " 'have',\n",
       " 'powder',\n",
       " 'laugh',\n",
       " 'have',\n",
       " 'run',\n",
       " 'peep',\n",
       " 'be',\n",
       " 'go',\n",
       " 'be',\n",
       " 'sit',\n",
       " 'star',\n",
       " 'go',\n",
       " 'knock',\n",
       " 'say',\n",
       " 'be',\n",
       " 'get',\n",
       " 'be',\n",
       " 'look',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'think',\n",
       " 'ask',\n",
       " 'be',\n",
       " 'do',\n",
       " 'be',\n",
       " 'tell',\n",
       " 'say',\n",
       " 'talk',\n",
       " 'say',\n",
       " 'say',\n",
       " 'sneeze',\n",
       " 'tell',\n",
       " 'say',\n",
       " 'wasnot',\n",
       " 'be',\n",
       " 'speak',\n",
       " 'say',\n",
       " 'quitejumped',\n",
       " 'saw',\n",
       " 'be',\n",
       " 'address',\n",
       " 'thebaby',\n",
       " 'take',\n",
       " 'go',\n",
       " 'didn',\n",
       " 'know',\n",
       " 'grin',\n",
       " 'didn',\n",
       " 'tknow',\n",
       " '_could_',\n",
       " 'don',\n",
       " 'do',\n",
       " 'say',\n",
       " 'feel',\n",
       " 'quitepleased',\n",
       " 'have',\n",
       " 'get',\n",
       " 'do',\n",
       " 'think',\n",
       " 'wouldbe',\n",
       " 'introduce',\n",
       " 'cry',\n",
       " 'jump',\n",
       " 'downin',\n",
       " '_not_',\n",
       " 'be',\n",
       " 'say',\n",
       " 'felt',\n",
       " 'toget',\n",
       " 'show',\n",
       " 'glance',\n",
       " 'see',\n",
       " 'mean',\n",
       " 'takethe',\n",
       " 'be',\n",
       " 'stir',\n",
       " 'seem',\n",
       " 'go',\n",
       " '_think_',\n",
       " 'oris',\n",
       " 'twelve',\n",
       " 'sing',\n",
       " 'keep',\n",
       " 'howl',\n",
       " 'hear',\n",
       " 'speak',\n",
       " 'beat',\n",
       " 'sneeze',\n",
       " 'thoroughly',\n",
       " 'enjoy',\n",
       " 'please',\n",
       " 'say',\n",
       " 'fling',\n",
       " 'speak',\n",
       " 'catch',\n",
       " 'be',\n",
       " 'hold',\n",
       " 'don',\n",
       " 'take',\n",
       " 'kill',\n",
       " 'bemurder',\n",
       " 'leave',\n",
       " 'say',\n",
       " 'ofexpressing',\n",
       " 'grunt',\n",
       " 'look',\n",
       " 'see',\n",
       " 'be',\n",
       " 'be',\n",
       " 'hada',\n",
       " 'be',\n",
       " 'get',\n",
       " 're',\n",
       " 'go',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'have',\n",
       " 'do',\n",
       " 'be',\n",
       " 'begin',\n",
       " 'think',\n",
       " 'be',\n",
       " 'dowith',\n",
       " 'get',\n",
       " 'grin',\n",
       " 'saw',\n",
       " 'please',\n",
       " 'go',\n",
       " 'don',\n",
       " 'say',\n",
       " 'so',\n",
       " 'get',\n",
       " 'add',\n",
       " 'felt',\n",
       " 'be',\n",
       " 'deny',\n",
       " 'try',\n",
       " 'don',\n",
       " 'want',\n",
       " 'go',\n",
       " 'remark',\n",
       " 'say',\n",
       " 'think',\n",
       " 'prove',\n",
       " 'go',\n",
       " 'know',\n",
       " 're',\n",
       " 'suppose',\n",
       " 'say',\n",
       " 'purr',\n",
       " 'growl',\n",
       " 'say',\n",
       " 'like',\n",
       " 'say',\n",
       " 'haven',\n",
       " 'be',\n",
       " 'be',\n",
       " 'surprise',\n",
       " 'be',\n",
       " 'get',\n",
       " 'use',\n",
       " 'happen',\n",
       " 'turn',\n",
       " 'say',\n",
       " 'have',\n",
       " 'come',\n",
       " 'wait',\n",
       " 'expect',\n",
       " 'see',\n",
       " 'do',\n",
       " 'walk',\n",
       " 'inwhich',\n",
       " 'be',\n",
       " 'say',\n",
       " 'live',\n",
       " 'say',\n",
       " 'reply',\n",
       " 'wish',\n",
       " 'wouldn',\n",
       " 'keep',\n",
       " 'vanish',\n",
       " 'make',\n",
       " 'see',\n",
       " 'think',\n",
       " 'agrin',\n",
       " 'think',\n",
       " 'suppose',\n",
       " 'doesn',\n",
       " 'say',\n",
       " 'sit',\n",
       " 'look',\n",
       " 'round',\n",
       " 'be',\n",
       " 'wasn',\n",
       " 'offer',\n",
       " 'say',\n",
       " 'didn',\n",
       " 'know',\n",
       " 'be',\n",
       " 'say',\n",
       " 'lay',\n",
       " 'learn',\n",
       " 'make',\n",
       " 'say',\n",
       " 'think',\n",
       " 'say',\n",
       " 'do',\n",
       " 'reply',\n",
       " 'mean',\n",
       " 'know',\n",
       " 'say',\n",
       " 'turn',\n",
       " 'have',\n",
       " 'take',\n",
       " 'be',\n",
       " 'look',\n",
       " 'shake',\n",
       " 'hold',\n",
       " 'consider',\n",
       " 'say',\n",
       " 'have',\n",
       " 'be',\n",
       " 'look',\n",
       " 'reply',\n",
       " 'itstays',\n",
       " 'felt',\n",
       " 'puzzle',\n",
       " 'give',\n",
       " 'reply',\n",
       " 'sigh',\n",
       " 'don',\n",
       " 'know',\n",
       " 'mean',\n",
       " 'say',\n",
       " 'reply',\n",
       " 'know',\n",
       " 'have',\n",
       " 'beattime',\n",
       " 'learn',\n",
       " 'be',\n",
       " 'say',\n",
       " 'be',\n",
       " 'know',\n",
       " 'ask',\n",
       " 'say',\n",
       " 'exclaim',\n",
       " 'come',\n",
       " 'say',\n",
       " 'don',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs = []\n",
    "for word_arr in verbdict:\n",
    "    for word in word_arr:\n",
    "        word = re.sub(r\"[^\\w\\s]\", \"\", word, re.UNICODE)\n",
    "        if word and word != \"s\":\n",
    "            verbs.append(word)\n",
    "            \n",
    "verbs #глаголы вообще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66d9f319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 226),\n",
       " ('say', 182),\n",
       " ('have', 90),\n",
       " ('think', 67),\n",
       " ('go', 51),\n",
       " ('do', 41),\n",
       " ('get', 39),\n",
       " ('look', 33),\n",
       " ('see', 28),\n",
       " ('begin', 27)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = FreqDist(nltk.Text(verbs))\n",
    "fdist.most_common(10)\n",
    "#наиболее часто встречающиеся глаголы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4adea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
